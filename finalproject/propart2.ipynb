{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QU2G9KqsNm6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pydrive.auth import GoogleAuth \n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "import logging\n",
    "logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "poX4tNuluHQr",
    "outputId": "2c393fae-e2e9-40a4-e016-e8acd02172d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: More, id: 1LbKM_iESxb5rpTZLZAwnWASOCRMpfK7x\n",
      "title: labels_with_bbox.json, id: 1oaXHHoTR0lea-b73DynERdM6z889TSa1\n",
      "title: راهنمای استفاده از داده ها.pdf, id: 1FM-1vUvkcBzmXYvjV1cRDfVBUgVeduWr\n",
      "title: labels.json, id: 1XVwbPFCHKHmgPTZPRRcAGS3WYXTIciCz\n",
      "title: images.zip, id: 1b6bnYM6_qsJA2Upezl20E9Yk_crIKgCB\n",
      "title: captions.json, id: 1dAyIrlkez3GsluFAddpuo4YTM9L7Xw0U\n",
      "title: categories_info.json, id: 1LrSfF57Ia5iB_4o4tygCieg1wA_QRb4R\n",
      "title: images_info.json, id: 17FIpIpvz7XsxV3FPtV5iNOGsdcfiSRP-\n"
     ]
    }
   ],
   "source": [
    "file_list = drive.ListFile({'q': \"'10Fs-dG_GyXIf0ZYKbmXcabvhFtayMbmt' in parents and trashed=false\"}).GetList()\n",
    "for file1 in file_list:\n",
    "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqrDG0youxmD"
   },
   "outputs": [],
   "source": [
    "data_downloaded = drive.CreateFile({'id': '1XVwbPFCHKHmgPTZPRRcAGS3WYXTIciCz'})\n",
    "data_downloaded.GetContentFile('labels.json')\n",
    "data_downloaded = drive.CreateFile({'id': '1b6bnYM6_qsJA2Upezl20E9Yk_crIKgCB'})\n",
    "data_downloaded.GetContentFile('images.zip')\n",
    "data_downloaded = drive.CreateFile({'id': '1dAyIrlkez3GsluFAddpuo4YTM9L7Xw0U'})\n",
    "data_downloaded.GetContentFile('captions.json')\n",
    "data_downloaded = drive.CreateFile({'id': '1LrSfF57Ia5iB_4o4tygCieg1wA_QRb4R'})\n",
    "data_downloaded.GetContentFile('categories_info.json')\n",
    "data_downloaded = drive.CreateFile({'id': '17FIpIpvz7XsxV3FPtV5iNOGsdcfiSRP-'})\n",
    "data_downloaded.GetContentFile('images_info.json')\n",
    "# data_downloaded = drive.CreateFile({'id': '1LbKM_iESxb5rpTZLZAwnWASOCRMpfK7x'})\n",
    "# data_downloaded.GetContentFile('More/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-BAj4W7zUl9n",
    "outputId": "3d9639e5-a3a3-460d-fcb4-dfb16be15252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "path = '/content/gdrive/My Drive/More/MoreImages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzH3reeyY_W3"
   },
   "outputs": [],
   "source": [
    "# Create a ZipFile Object and load sample.zip in it\n",
    "from zipfile import ZipFile\n",
    "with ZipFile('images.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in different directory\n",
    "   zipObj.extractall('images')\n",
    "with ZipFile('/content/gdrive/My Drive/More/MoreImages.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in different directory\n",
    "   zipObj.extractall('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0HEc_DliyPw"
   },
   "outputs": [],
   "source": [
    "def mergefolders(root_src_dir, root_dst_dir):\n",
    "    for src_dir, dirs, files in os.walk(root_src_dir):\n",
    "        dst_dir = src_dir.replace(root_src_dir, root_dst_dir, 1)\n",
    "        if not os.path.exists(dst_dir):\n",
    "            os.makedirs(dst_dir)\n",
    "        for file_ in files:\n",
    "            src_file = os.path.join(src_dir, file_)\n",
    "            dst_file = os.path.join(dst_dir, file_)\n",
    "            if os.path.exists(dst_file):\n",
    "                os.remove(dst_file)\n",
    "            shutil.copy(src_file, dst_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDmljFYYEcKL"
   },
   "source": [
    "**Merging All Photos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMa5fckEBVK-"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "mergefolders('/content/images/MoreImages','/content/images/val2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xQ3Mmi92aOq-",
    "outputId": "aad8663b-8c0a-4228-cc38-24d887899c50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of jpg flies in image: 15000\n"
     ]
    }
   ],
   "source": [
    "import sys, time, os, warnings \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "jpgs = os.listdir('images/val2017')\n",
    "print(\"The number of jpg flies in image: {}\".format(len(jpgs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GYWudS8hv2nv"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "with open('labels.json') as json_file: lab = json.load(json_file)\n",
    "with open('captions.json') as json_file: cap = json.load(json_file)\n",
    "with open('/content/gdrive/My Drive/More/additional_labels.json') as json_file: morelab = json.load(json_file)\n",
    "with open('/content/gdrive/My Drive/More/additional_captions.json') as json_file: morecap = json.load(json_file)\n",
    "c=pd.DataFrame(lab+morelab)\n",
    "c.pop('bbox')\n",
    "f={}\n",
    "for name in jpgs: \n",
    "  if (c['image_id']==int(name[0:12])).any():\n",
    "    f.update({str(int(name[0:12])):c.loc[c['image_id']==int(name[0:12]),'category_id'].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KKezUD1tbdQ5",
    "outputId": "7d2cdf58-4d12-4e83-fba8-125b2d869ad7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14867"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c.pop('bbox')\n",
    "len(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QFwTv8rlE7LM",
    "outputId": "b79c47d6-fd3c-4fcb-a65b-db23e57a9d31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jpgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "iHTl8qJh9iAM",
    "outputId": "09d9cc66-dd68-48ac-aa6a-3b034f73bfae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     A black Honda motorcycle parked in front of a ...\n",
       "1         A Honda motorcycle parked in a grass driveway\n",
       "5     A black Honda motorcycle with a dark burgundy ...\n",
       "44    Ma motorcycle parked on the gravel in front of...\n",
       "46    A motorcycle with its brake extended standing ...\n",
       "Name: caption, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('captions.json') as json_file: cap = json.load(json_file)\n",
    "with open('/content/gdrive/My Drive/More/additional_captions.json') as json_file: morecap = json.load(json_file)\n",
    "cap=pd.DataFrame(cap+morecap)\n",
    "cap[cap['image_id']==179765]['caption']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vsCfWTqEiIy"
   },
   "source": [
    "**Creating dictionary of captions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YpvL_fmIvmiL"
   },
   "outputs": [],
   "source": [
    "descriptions=dict()\n",
    "for name in jpgs: \n",
    "  v=list()\n",
    "  for i in cap[cap['image_id']==int(name[0:12])]['caption']:\n",
    "    v.append(i)\n",
    "  descriptions.update({str(int(name[0:12])):v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "mh2Glq_sf1jE",
    "outputId": "7d204af9-2b3f-49a9-8dc8-75d5a71facf1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yLcLRv-RdKtL",
    "outputId": "fc5002bc-a7c1-4a82-cbf1-6cd4ff4c8c87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(descriptions.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "loslTI7bsCAn"
   },
   "source": [
    "# Storing captions and image names in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMQSJrnyjRUE"
   },
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "for key , values in descriptions.items():\n",
    "\n",
    "  for ca in values:\n",
    "    caption = '<start> ' + ca + ' <end>'\n",
    "    image_id = int(key)\n",
    "    full_coco_image_path = '/content/images/val2017'+'/' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_name_vector.append(full_coco_image_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                          all_img_name_vector,\n",
    "                                          random_state=1)\n",
    "\n",
    "# # Select the first 75043 captions from the shuffled set\n",
    "train_captions = train_captions\n",
    "img_name_vector = img_name_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mHRtgGKSeiPf",
    "outputId": "850ff6f1-0223-4c1a-f604-f38891d0cd79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75043"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mqTQzsSlcvUw"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "GrDsF0WncvYS",
    "outputId": "54095791-9d32-4628-f960-d6345421bbb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vuFrpWh5hLE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BRcmX__cvba"
   },
   "outputs": [],
   "source": [
    "# Get unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in image_dataset:\n",
    "  batch_features = image_features_extract_model(img)\n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "  for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    np.save(path_of_feature, bf.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Df8eUu5gcveN"
   },
   "outputs": [],
   "source": [
    "# maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlFeDWsWcvgz"
   },
   "outputs": [],
   "source": [
    "# 6000 popular  words are chosen from the vocabulary\n",
    "top_k = 6000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BN31t4JZcvke"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DuDcwl7cvoF"
   },
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4VqEvHXE6kS"
   },
   "source": [
    "**Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aWEu6PScvto"
   },
   "outputs": [],
   "source": [
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMEEPeygcvwJ"
   },
   "outputs": [],
   "source": [
    "#  max_length  which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcH56RZxAWv1"
   },
   "source": [
    "# 80-20 split training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zg4NzsgEcvy2",
    "outputId": "f902247b-7aec-4f03-ec91-27c41b98f6bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60034, 60034, 15009, 15009)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
    "                                                                    cap_vector,\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sc59jJf1AJL0"
   },
   "source": [
    "**Model hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nblCyk3UcvsH"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPh_KGMEBEVN"
   },
   "source": [
    "Load the numpy(feature) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwzIXnfsfwo7"
   },
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "  return img_tensor, cap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfvTlSxIfwsT"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3nT90eMD37p"
   },
   "source": [
    "**Attention model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "maRki3K9fwxG"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "# features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "# hidden shape == (batch_size, hidden_size)\n",
    "# hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "  def call(self, features, hidden):\n",
    "\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, 64, hidden_size)\n",
    "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5u_I935zwjVR"
   },
   "source": [
    "**Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaJS7yG5fwvO"
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # This encoder passes extracted features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-by39IowdOY"
   },
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydJgRTTLcvqZ"
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7275gdfJgBw6"
   },
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8WLkIR9FAU4"
   },
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l02V7ymxgB0v"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1KXoVXAgB5i"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./content/gdrive/My Drive/saving\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlkW9trEgB9A"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6M0rPrARwrct"
   },
   "source": [
    "**Loss structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gd9_bWmmcvm3"
   },
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch as captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # piping extracted features through the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # teacher forcing\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bi5gaJ6Ww8zB"
   },
   "source": [
    "**Loss visulization and formulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pIb2q3picvjV",
    "outputId": "fb5c45fd-9971-4ef1-c460-99bf264e4735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1 Batch 0 Loss 1.9864\n",
      "Epoch 1 Batch 100 Loss 1.0421\n",
      "Epoch 1 Batch 200 Loss 0.9378\n",
      "Epoch 1 Batch 300 Loss 0.9283\n",
      "Epoch 1 Batch 400 Loss 0.8482\n",
      "Epoch 1 Batch 500 Loss 0.8582\n",
      "Epoch 1 Batch 600 Loss 0.7988\n",
      "Epoch 1 Batch 700 Loss 0.7914\n",
      "Epoch 1 Batch 800 Loss 0.7552\n",
      "Epoch 1 Batch 900 Loss 0.6832\n",
      "Epoch 1 Loss 0.870838\n",
      "Time taken for 1 epoch 852.9357860088348 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.7483\n",
      "Epoch 2 Batch 100 Loss 0.6864\n",
      "Epoch 2 Batch 200 Loss 0.6812\n",
      "Epoch 2 Batch 300 Loss 0.7460\n",
      "Epoch 2 Batch 400 Loss 0.7131\n",
      "Epoch 2 Batch 500 Loss 0.7368\n",
      "Epoch 2 Batch 600 Loss 0.6966\n",
      "Epoch 2 Batch 700 Loss 0.7071\n",
      "Epoch 2 Batch 800 Loss 0.6787\n",
      "Epoch 2 Batch 900 Loss 0.6107\n",
      "Epoch 2 Loss 0.689028\n",
      "Time taken for 1 epoch 1163.726729631424 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.6689\n",
      "Epoch 3 Batch 100 Loss 0.6151\n",
      "Epoch 3 Batch 200 Loss 0.6231\n",
      "Epoch 3 Batch 300 Loss 0.6892\n",
      "Epoch 3 Batch 400 Loss 0.6555\n",
      "Epoch 3 Batch 500 Loss 0.6791\n",
      "Epoch 3 Batch 600 Loss 0.6430\n",
      "Epoch 3 Batch 700 Loss 0.6570\n",
      "Epoch 3 Batch 800 Loss 0.6296\n",
      "Epoch 3 Batch 900 Loss 0.5782\n",
      "Epoch 3 Loss 0.635027\n",
      "Time taken for 1 epoch 1181.3905456066132 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6188\n",
      "Epoch 4 Batch 100 Loss 0.5763\n",
      "Epoch 4 Batch 200 Loss 0.5830\n",
      "Epoch 4 Batch 300 Loss 0.6479\n",
      "Epoch 4 Batch 400 Loss 0.6218\n",
      "Epoch 4 Batch 500 Loss 0.6388\n",
      "Epoch 4 Batch 600 Loss 0.6139\n",
      "Epoch 4 Batch 700 Loss 0.6280\n",
      "Epoch 4 Batch 800 Loss 0.5972\n",
      "Epoch 4 Batch 900 Loss 0.5551\n",
      "Epoch 4 Loss 0.603727\n",
      "Time taken for 1 epoch 1176.9871773719788 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.5898\n",
      "Epoch 5 Batch 100 Loss 0.5581\n",
      "Epoch 5 Batch 200 Loss 0.5561\n",
      "Epoch 5 Batch 300 Loss 0.6233\n",
      "Epoch 5 Batch 400 Loss 0.5801\n",
      "Epoch 5 Batch 500 Loss 0.6037\n",
      "Epoch 5 Batch 600 Loss 0.5774\n",
      "Epoch 5 Batch 700 Loss 0.5962\n",
      "Epoch 5 Batch 800 Loss 0.5605\n",
      "Epoch 5 Batch 900 Loss 0.5365\n",
      "Epoch 5 Loss 0.571742\n",
      "Time taken for 1 epoch 1179.3819139003754 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.5618\n",
      "Epoch 6 Batch 100 Loss 0.5373\n",
      "Epoch 6 Batch 200 Loss 0.5279\n",
      "Epoch 6 Batch 300 Loss 0.6002\n",
      "Epoch 6 Batch 400 Loss 0.5434\n",
      "Epoch 6 Batch 500 Loss 0.5765\n",
      "Epoch 6 Batch 600 Loss 0.5477\n",
      "Epoch 6 Batch 700 Loss 0.5719\n",
      "Epoch 6 Batch 800 Loss 0.5254\n",
      "Epoch 6 Batch 900 Loss 0.5164\n",
      "Epoch 6 Loss 0.545530\n",
      "Time taken for 1 epoch 1178.1898093223572 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.5395\n",
      "Epoch 7 Batch 100 Loss 0.5230\n",
      "Epoch 7 Batch 200 Loss 0.5008\n",
      "Epoch 7 Batch 300 Loss 0.5735\n",
      "Epoch 7 Batch 400 Loss 0.5185\n",
      "Epoch 7 Batch 500 Loss 0.5533\n",
      "Epoch 7 Batch 600 Loss 0.5258\n",
      "Epoch 7 Batch 700 Loss 0.5532\n",
      "Epoch 7 Batch 800 Loss 0.5051\n",
      "Epoch 7 Batch 900 Loss 0.5016\n",
      "Epoch 7 Loss 0.524417\n",
      "Time taken for 1 epoch 1178.857766866684 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.5266\n",
      "Epoch 8 Batch 100 Loss 0.5102\n",
      "Epoch 8 Batch 200 Loss 0.4830\n",
      "Epoch 8 Batch 300 Loss 0.5498\n",
      "Epoch 8 Batch 400 Loss 0.4932\n",
      "Epoch 8 Batch 500 Loss 0.5355\n",
      "Epoch 8 Batch 600 Loss 0.5040\n",
      "Epoch 8 Batch 700 Loss 0.5238\n",
      "Epoch 8 Batch 800 Loss 0.4829\n",
      "Epoch 8 Batch 900 Loss 0.4889\n",
      "Epoch 8 Loss 0.504107\n",
      "Time taken for 1 epoch 1180.2132408618927 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.5121\n",
      "Epoch 9 Batch 100 Loss 0.5062\n",
      "Epoch 9 Batch 200 Loss 0.4679\n",
      "Epoch 9 Batch 300 Loss 0.5298\n",
      "Epoch 9 Batch 400 Loss 0.4722\n",
      "Epoch 9 Batch 500 Loss 0.5088\n",
      "Epoch 9 Batch 600 Loss 0.4895\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pn2Hl-O-42vy"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WpeLw1looN_Z"
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3mGhlv1oOQO"
   },
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R-7gMGZVFWg-"
   },
   "source": [
    "**# captions on the validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8dM0NrDNoVWb"
   },
   "outputs": [],
   "source": [
    "\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-D9PAYLCAsMK"
   },
   "outputs": [],
   "source": [
    "len(jpgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UsWT17qgSs-Q"
   },
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDudsHFLSrwe"
   },
   "outputs": [],
   "source": [
    "result, attention_plot = evaluate(image_path)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image_path, result, attention_plot)\n",
    "# opening the image\n",
    "Image.open(image_path)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled(9)(8).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
